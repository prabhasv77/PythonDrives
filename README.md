# PythonDrives
Creating a Python project than helps us maneuver a vehicle inside a simulation. Enhancing the performance of Python model by providing analog Input through the program instead of Digital Input

# INITIAL APPROACH
The initial approach was to read the the screen using OPENCV and descale the resolution and FPS so the images can be processed at faster rate. After the image is captured using openCV it is then converted to greyscale. The greyscale image is applied with CV2 Canny function for better edge detection. and the lanes present on the surface of the road are marked using HOUGHLINES Transform. The steering input is altered based on the orientation of the Hough Lines present on the lanes and correction are made to stop the vehicle from crashing or going off the road.
## DRAWBACKS
This approach only works for slow vehicles and cannot avoid collisions when traffic.

# Datasets
The custom dataset is generated by driving the car from a first-person perspective view, which allowed the data to be trained on actual vehicle vision. This is done by turning on the hood camera in GTA-5. However, due to limited ram and free space in the System, the data is collected in grayscale images with a width of 160 pixels and a height of 120 pixels [160√ó120√óùüè]. Each image in the dataset has a respective steering angle and throttle. After driving the vehicle for a few hours in GTA-5, a dataset with 100,000 labeled images is collected. This dataset is then uploaded on Google Drive, which is mounted with GoogleColab for preprocessing and training.

The figure below shows the distribution of the steering angle and throttle of the collected dataset. As the car is driven straight for most of the part, most of the collected data lie about steering angle 0. Training a neural network on such a dataset would result in biased prediction to drive straight all the time, resulting in a poor performance by the network.
A threshold of 4000 is used to balance the steering data and throttle data to provide us with a more balanced data.

![steering](https://github.com/prabhasv77/PythonDrives/assets/120770931/64c47881-c2ba-43db-8b6e-7d585884eadf)

## ARTIFICIALLY EXPANDING THE DATASET
Training the CNN on 39,046 images might not be enough, therefore the balanced dataset is artificially expended using image augmentation. Each of the images in the balanced dataset is flipped along the horizontal axis, and the associated steering angle of each of the images is multiplied with -1 to get the flipped steering angle. These augmented images are then added to the balanced dataset, resulting in artificially expanding the size of the dataset to 78,092. 

![flip](https://github.com/prabhasv77/PythonDrives/assets/120770931/d5fd4d4e-e95f-4a7f-938f-4e676983eb59)

# CNN MODEL USED FOR THE PROJECT
NVIDIA‚Äôs PilotNet, ResNet-50, and VGG-19 are trained on a self-generated dataset and dataset upload by Sentdex on his website. The dataset is divided into 25 npy files each of size 100MB. These trained networks are then tested under different environments, such as highway with/without traffic and city with/without traffic. Due to fast prediction in a second (fps), PilotNet is selected for this project. The training data was collected by driving and recording the frames from the camera and steering angle from the steering wheel in various weather and road conditions. The collected data was fed into proposed CNN. The weight and biases in the CNN were adjusted, by minimizing the error function (mean square error) through backpropagation. After the proposed network is successfully trained, the image from the hood camera is passed into the network to get the corresponding steering angle to drive the car.

## PILOTNET ARCHITECTURE
The PilotNet‚Äôs consists of 9 layers; 5 convolutional layers, 3 fully connected layers, and one normalization layer. The features are extracted in the convolutional layer with a stride of 2x2 and kernel filter of 5x5 in the first three convolutional layers, and kernel filter of 3x3 in the last two convolutional layers. These extracted features map is passed into a fully connected deep neural network which predicts the output steering angle. The following shows the order of convolutional layers, pooling layers, and fully connected layers in NVIDIA‚Äôs PilotNet.

![architecture](https://github.com/prabhasv77/PythonDrives/assets/120770931/0dfa11fc-ad14-470d-bfdc-3cb30e079ee2)

# INTERGRATING PILOTNET with YOLOv3
Pretrained YOLOv3 and YOLOv3-Tiny, on the COCO dataset, are used to detect such objects in real-time. The COCO dataset has 80 object classes, however for this application, detected objects belonging to car, truck, person, motorcycle, bus, train or stop sign classes are considered. Other objects such as zebra, apple, oranges, etc. are ignored. YOLOv3-Tiny is used for this application due to faster prediction compared to YOLOv3. It is also important to know the distance between the detected object and the self-driving car to slow or brake accordingly. This is obtained by calculating the diagonal distance using tw, th, of the detected object, Both PilotNet and YOLOv3-Tiny are integrated into a single script to control the steering and throttle. If the detected object is too closed, the predicted throttle value is changed to -1 (no acceleration), the predicted steering angle is changed to 0 (no steering), and the brake is applied. If the detected object is nearby, but not too closely, the predicted throttle value decreased significantly to slow down the self-driving vehicle. If the predicted vehicle far, the predicted throttle value is decreased by a small amount.

# USING VJOY AND XBOX EMULATOR
Now, to take a step furthur and improve the performance of driving model. The simulation is fed with analog input through an xbox emulator VJOY. As, many of you know joystick has analog input unlike digital input of keyboard. So the program is modified to output analog values of throttle data and steering data and fed to the simulation through vjoy since keyboard cannnot send analog input. The analog input helps for more accurate corrections to throttle data and steering data.

